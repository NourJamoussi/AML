{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"> # Setting up the notebook","metadata":{"papermill":{"duration":0.028809,"end_time":"2021-05-12T16:22:14.91115","exception":false,"start_time":"2021-05-12T16:22:14.882341","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport nltk\nfrom sklearn.model_selection import train_test_split\nfrom wordcloud import WordCloud\nfrom nltk.corpus import stopwords\nfrom PIL import Image\nimport re\nplt.style.use('ggplot')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/eurecom-aml-2022-challenge-3/train.csv')\ntest_df = pd.read_csv('/kaggle/input/eurecom-aml-2022-challenge-3/test.csv')","metadata":{"execution":{"iopub.execute_input":"2021-05-12T16:22:14.976876Z","iopub.status.busy":"2021-05-12T16:22:14.976021Z","iopub.status.idle":"2021-05-12T16:22:15.167016Z","shell.execute_reply":"2021-05-12T16:22:15.167541Z"},"papermill":{"duration":0.226062,"end_time":"2021-05-12T16:22:15.167724","exception":false,"start_time":"2021-05-12T16:22:14.941662","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> # EDA - Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"print(train_df.info())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test_df.info())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Train dataset has {len(train_df)} records and {train_df.isna().sum().sum()} null values\" )\nprint(f\"Test dataset has {len(test_df)} records and {test_df.isna().sum().sum()} null values\" )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# How many unique text IDs ?\nlen(train_df[\"textID\"].unique())  # 24732 -> no duplicates IDs","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# How many unique texts ?\nlen(train_df[\"text\"].unique())  # 24732 -> no duplicates texts","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,6))\nsns.histplot(data=train_df, x=\"sentiment\", palette=\"mako\")\nplt.title(\"Tweet sentiments - training data\", fontsize=20, pad=10)\nplt.xlabel(\"Sentiment\", fontsize=15, labelpad=10)\nplt.ylabel(\"Count\", fontsize=15, labelpad=10)\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_df = train_df.groupby('sentiment').count()['text'].reset_index().sort_values(by='text',ascending=False)\npercent_class=class_df.text\nlabels= class_df.sentiment\ncolors = ['#FACA0C','#17C37B','#F92969']\nplt.figure(figsize=(12,6))\nmy_pie,_,_ = plt.pie(percent_class, radius = 1, labels=labels, colors=colors, autopct=\"%.1f%%\", textprops={'fontsize': 16})\nplt.setp(my_pie, width=0.7, edgecolor='white') \nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Avgerage tweet's length: {train_df['text'].apply(len).mean():.2f} characters\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def count_words(text):\n  '''\n  Count the words in a text\n  '''\n  return len(text.split(\" \"))\n\ndf_text_sentiment = train_df.loc[:, [\"text\", \"sentiment\"]]\n# Map text to length in terms of characters\ndf_text_sentiment[\"number_of_characters\"] = df_text_sentiment[\"text\"].apply(lambda text : len(text))\ndf_text_sentiment[\"number_of_words\"] = df_text_sentiment[\"text\"].apply(count_words)\ndf_text_sentiment = df_text_sentiment.drop(columns=[\"text\"])\n\n# Visualize scatter plot with tweet's characters and sentiment\ncdict = {\"positive\": \"green\", \"neutral\": \"yellow\", \"negative\": \"red\"}\nplt.rcParams['axes.facecolor'] = 'white'\nplt.figure(figsize=(12,6))\nplt.scatter(x='number_of_words', y='number_of_characters', c=df_text_sentiment['sentiment'].map(cdict), data=df_text_sentiment)\n#plt.title(\"Correlation between #words , #characters and the sentiment\", fontsize=20, pad=10)\nplt.xlabel(\"#words\", fontsize=15 , color='black')\nplt.ylabel(\"#characters\", fontsize=15, color='black')\nplt.xticks(fontsize=10, color='black')\nplt.yticks(fontsize=10, color='black')\nplt.grid(color='black', linestyle='-.', linewidth=0.7)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot stacked bar plot\ncross_tab_prop = pd.crosstab(index=df_text_sentiment['number_of_words'],\n                             columns=df_text_sentiment['sentiment'],\n                             normalize=\"index\")\ncross_tab_prop.plot(kind='bar', \n                    stacked=True, \n                    figsize=(15, 8),\n                    color=['#F92969','#FACA0C','#17C37B'])\n\nplt.legend(loc=\"upper left\", ncol=3)\nplt.xlabel(\"Number of words\")\nplt.ylabel(\"Sentiment proportion\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,(ax1,ax2,ax3) = plt.subplots(1,3,figsize=(15,5))\n\ntweet_len=train_df[train_df['sentiment']==\"positive\"]['text'].str.len()\nax1.hist(tweet_len,color='#17C37B')\nax1.set_title('Positive Sentiments')\nax1.set_ylabel(\"Count\", fontsize=15)\n\ntweet_len=train_df[train_df['sentiment']==\"neutral\"]['text'].str.len()\nax2.hist(tweet_len,color='#FACA0C')\nax2.set_title('Neutral Sentiments')\nax2.set_xlabel(\"Number of characters\", fontsize=15)\n\ntweet_len=train_df[train_df['sentiment']==\"negative\"]['text'].str.len()\nax3.hist(tweet_len,color='#F92969')\nax3.set_title('Negative Sentiments')\n\nfig.suptitle('Characters in a tweet', fontsize=20)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,(ax1,ax2,ax3)=plt.subplots(1,3,figsize=(15,5))\n\ntweet_len=train_df[train_df['sentiment']==\"positive\"]['text'].str.split().map(lambda x: len(x))\nax1.hist(tweet_len,color='#17C37B')\nax1.set_title('Positive Sentiments')\nax1.set_ylabel(\"Count\", fontsize=15)\n\n\ntweet_len=train_df[train_df['sentiment']==\"neutral\"]['text'].str.split().map(lambda x: len(x))\nax2.hist(tweet_len,color='#FACA0C')\nax2.set_title('Neutral Sentiments')\nax2.set_xlabel(\"Number of words\", fontsize=15)\n\n\ntweet_len=train_df[train_df['sentiment']==\"negative\"]['text'].str.split().map(lambda x: len(x))\nax3.hist(tweet_len,color='#F92969')\nax3.set_title('Negative Sentiments')\n\n\nfig.suptitle('Words in a tweet', fontsize=20)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def avg_words(sentiment):\n    mean_num_words = train_df[train_df[\"sentiment\"] == sentiment]['text'].str.split().map(lambda x: len(x)).mean()\n    return round(mean_num_words, 2)\n\n# Average tweet's length (in words) per sentiment\nprint(f\"Average number of words in positive tweets: {avg_words('positive')}\")\nprint(f\"Average number of words in neutral tweets: {avg_words('neutral')}\")\nprint(f\"Average number of words in negative tweets: {avg_words('negative')}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nltk.download(\"stopwords\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import Counter\n\ndef get_most_common_words(df):\n    text_positive = train_df[train_df[\"sentiment\"] == \"positive\"][\"text\"]\n    text_neutral = train_df[train_df[\"sentiment\"] == \"neutral\"][\"text\"]\n    text_negative = train_df[train_df[\"sentiment\"] == \"negative\"][\"text\"]\n\n    # capture words in text per sentiment\n    def get_mcw_per_sentiment(text):\n        comment_words = ''\n        for text_i in text_positive:\n            tokens = text_i.split()\n            for i in range(len(tokens)):\n                tokens[i] = tokens[i].lower()\n      \n        comment_words += \" \".join(tokens)+\" \"\n    \n        comment_words = comment_words.split(\" \")\n        # remove stopwords\n        comment_words = [x for x in comment_words if x not in stopwords.words(\"english\")]\n        return comment_words\n\n    # top 25 common words per sentiment, no stopwords\n    th = 25\n    words_positive = list(map(lambda x: x[0], Counter(get_mcw_per_sentiment(text_positive)).most_common(th)))\n    words_neutral = list(map(lambda x: x[0], Counter(get_mcw_per_sentiment(text_neutral)).most_common(th)))\n    words_negative = list(map(lambda x: x[0], Counter(get_mcw_per_sentiment(text_negative)).most_common(th)))\n\n    # intersection to see which are the most common words in general\n    return set(words_positive) & set(words_neutral) & set(words_negative)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_word_cloud(sentiment, color_palette, stopwords_list):\n    comment_words = ''\n    # iterate through the csv file\n    for val, curr_sentiment in zip(train_df[\"text\"], train_df[\"sentiment\"]):\n        if curr_sentiment != sentiment:\n            continue\n        # typecaste each val to string\n        val = str(val)\n        # split the value\n        tokens = val.split()\n        # Converts each token into lowercase\n        for i in range(len(tokens)):\n          tokens[i] = tokens[i].lower()\n    \n    comment_words += \" \".join(tokens)+\" \"\n\n    mask_dir = np.array(Image.open('twitter_logo.png'))\n    wordcloud = WordCloud(width = 800, height = 800,\n                        background_color ='white',\n                        stopwords = stopwords_list,\n                        min_font_size = 10,\n                        colormap=color_palette,\n                        mask=mask_dir).generate(comment_words)\n\n    return wordcloud\n\nstopwords_list = set(stopwords.words(\"english\")).union(set(get_most_common_words(train_df).union({\"lol\", \"work\", \"today\"})))\n# Plot Tweet's wordclouds per sentiment\nfig, axs = plt.subplots(nrows=1, ncols=3, figsize=(15, 8))\t\t\t\n# plt.figure(figsize = (8, 8), facecolor = None)\naxs[0].imshow(get_word_cloud(\"positive\", \"Greens\", stopwords_list))\naxs[0].axis(\"off\")\naxs[0].set_title(\"Positive wordcloud\", fontsize=20, pad=15)\n\naxs[1].imshow(get_word_cloud(\"neutral\", \"Wistia\", stopwords_list))\naxs[1].axis(\"off\")\naxs[1].set_title(\"Neutral wordcloud\", fontsize=20, pad=15)\n\naxs[2].imshow(get_word_cloud(\"negative\", \"Reds\", stopwords_list))\naxs[2].axis(\"off\")\naxs[2].set_title(\"Negative wordcloud\", fontsize=20, pad=15)\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Util function to get the tweets corpus of a specific sentiment\ndef create_corpus(sentiment):\n    corpus=[]\n    \n    for x in train_df[train_df['sentiment']==sentiment ]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import defaultdict\nimport string\n\n\n# Check punctuactions per sentiment\ndef plot_punctuactions(sentiment, color):\n    plt.figure(figsize=(10,5))\n    corpus=create_corpus(sentiment)\n    dic=defaultdict(int)\n    special = string.punctuation\n    for i in (corpus):\n        if i in special:\n            dic[i]+=1    \n    x,y=zip(*dic.items())\n    plt.title(f\"Punctuactions distribution in {sentiment} tweets\", fontsize=20, pad=10)\n    plt.ylabel(\"Count\", fontsize=15)\n    plt.xticks(fontsize=10)\n    plt.yticks(fontsize=10)\n    plt.bar(x,y,color=color)\n    plt.show()\n\nplot_punctuactions(\"positive\", \"#17C37B\")\nplot_punctuactions(\"neutral\", \"#FACA0C\")\nplot_punctuactions(\"negative\", \"#F92969\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check most common hashtags\ndef find_hashtag(text):\n    line=re.findall(r'(?<=#)\\w+',text)\n    return \" \".join(line)\n\ntrain_df['hash']=train_df['text'].apply(find_hashtag)\ntemp=train_df['hash'].value_counts()[:][1:13]\ntemp= temp.to_frame().reset_index().rename(columns={'index':'Hashtag','hash':'count'})\nplt.figure(figsize=(15,8))\nsns.barplot(x=\"Hashtag\",y=\"count\", data = temp)\nplt.title(\"Most popular hashtags\", fontsize=20, pad=10)\nplt.xlabel(\"Hashtag\", fontsize=15)\nplt.ylabel(\"Count\", fontsize=15)\nplt.xticks(fontsize=10)\nplt.yticks(fontsize=10)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check mentions\n\ndef find_mentions(text):\n    line=re.findall(r'(?<=@)\\w+',text)\n    return \" \".join(line)\ntrain_df['mentions']=train_df['text'].apply(find_mentions)\n\ntemp=train_df['mentions'].value_counts()[:][1:13]\ntemp =temp.to_frame().reset_index().rename(columns={'index':'Mentions','mentions':'count'})\n\nplt.figure(figsize=(15,8))\nsns.barplot(x=\"Mentions\",y=\"count\", data = temp)\nplt.title(\"Most popular mentions\", fontsize=20, pad=10)\nplt.xlabel(\"Mention\", fontsize=15)\nplt.ylabel(\"Count\", fontsize=15)\nplt.xticks(fontsize=10)\nplt.yticks(fontsize=10)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check tweets with very few characters - Some of them make no sense, check sentiment of those as well\ndef check_characters_length(df):\n    for i, text in enumerate(df[\"text\"]):\n        curr_length = len(text)\n        if curr_length <= 5:\n            print(f\"{text} - {df.loc[i, 'sentiment']}\")\n\ncheck_characters_length(train_df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n> # Preprocessing\n\n","metadata":{}},{"cell_type":"code","source":"# Remove URLs and HTML links\ndef remove_urls(text):\n    url_remove = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url_remove.sub(r'', text)\n\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\n# Remove mentions\ndef remove_mention(x):\n    return re.sub(r'@\\w+','',x)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install wordninja  # library we use to split concatenated words in hashtags\n\nimport wordninja","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip3 install pyenchant\n!sudo apt-get install libenchant1c2a\n\nimport enchant","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"abbreviations_dict = {\n  \"AFAIK\":\"As Far As I Know\",\n  \"AFK\":\"Away From Keyboard\",\n  \"ASAP\":\"As Soon As Possible\",\n  \"ATK\":\"At The Keyboard\",\n  \"ATM\":\"At The Moment\",\n  \"A3\":\"Anytime, Anywhere, Anyplace\",\n  \"BAK\":\"Back At Keyboard\",\n  \"BBL\":\"Be Back Later\",\n  \"BBS\":\"Be Back Soon\",\n  \"BFN\":\"Bye For Now\",\n  \"B4N\":\"Bye For Now\",\n  \"BRB\":\"Be Right Back\",\n  \"BRT\":\"Be Right There\",\n  \"BTW\":\"By The Way\",\n  \"B4\":\"Before\",\n  \"B4N\":\"Bye For Now\",\n  \"CUL8R\":\"See You Later\",\n  \"CYA\":\"See You\",\n  \"FAQ\":\"Frequently Asked Questions\",\n  \"FC\":\"Fingers Crossed\",\n  \"FWIW\":\"For What It's Worth\",\n  \"FYI\":\"For Your Information\",\n  \"GMTA\":\"Great Minds Think Alike\",\n  \"GR8\":\"Great!\",\n  \"G9\":\"Genius\",\n  \"ICQ\":\"I Seek you (also a chat program)\",\n  \"ILU\":\"ILU: I Love You\",\n  \"IMHO\":\"In My Honest Opinion\",\n  \"IOW\":\"In Other Words\",\n  \"KISS\":\"Keep It Simple, Stupid\",\n  \"LDR\":\"Long Distance Relationship\",\n  \"LMAO\":\"Laugh My fuck Off\",\n  \"LOL\":\"Laughing Out Loud\",\n  \"LTNS\":\"Long Time No See\",\n  \"L8R\":\"Later\",\n  \"MTE\":\"My Thoughts Exactly\",\n  \"M8\":\"Mate\",\n  \"NRN\":\"No Reply Necessary\",\n  \"PITA\":\"Pain In The fuck\",\n  \"PRT\":\"Party\",\n  \"PRW\":\"Parents Are Watching\",\n  \"ROFL\":\"Rolling On The Floor Laughing\",\n  \"ROFLOL\":\"Rolling On The Floor Laughing Out Loud\",\n  \"ROTFLMAO\":\"Rolling On The Floor Laughing My fuck Off\",\n  \"SK8\":\"Skate\",\n  \"STATS\":\"Your sex and age\",\n  \"THX\":\"Thank You\",\n  \"TTYL\":\"Talk To You Later\",\n  \"U2\":\"You Too\",\n  \"U4E\":\"Yours For Ever\",\n  \"WB\":\"Welcome Back\",\n  \"WTF\":\"What The fuck\",\n  \"WTG\":\"Way To Go!\",\n  \"WUF\":\"Where Are You From?\",\n  \"WKDN\":\"Week-End\",\n  \"W8\":\"Wait\",\n  \"7K\":\"Sick Laugher\",\n  \"****\":\"fuck\"\n}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"abbreviations_dict = dict((k.lower(), v.lower()) for k, v in abbreviations_dict.items())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d = enchant.Dict(\"en_US\")\n\ndef handle_hashtag(x):\n    if \"#\" in x:\n        hashtags = re.findall(r\"#(\\w+)\",x)  # may have more than one hashtag per tweet\n        original_hashtags = hashtags.copy()\n        corrects = [0] * len(hashtags)\n        for i, hashtag in enumerate(hashtags):\n            hashtag_final = hashtag.lower()\n            # check if the hashtag is a english word, in case just remove the #\n            if d.check(hashtag_final):\n                corrects[i] = 1\n            else:\n                # check if there is some abbreviation in the hashtag, in case write it in complete form\n                for abbrev in abbreviations_dict.keys():\n                    if abbrev in hashtag:\n                        idx_start = hashtag.find(abbrev)\n                        hashtag_final = hashtag[:idx_start] + re.sub(\" \", \"\", abbreviations_dict[abbrev]) + hashtag[idx_start+len(abbrev):]\n                # split the hashtag with wordninja and remove the #\n                hashtag_final = \" \".join(wordninja.split(hashtag_final))\n                # check that each splitted word is in the english vocabulary, otherwise just delete the hashtag\n                corrects[i] = 1\n                for word in hashtag_final.split(\" \"):\n                    if not d.check(word): # and not d.check(word.capitalize()):  # also cover where i.e. friday is not found, but Friday is found\n                        corrects[i] = 0\n            if corrects[i]:\n                hashtags[i] = hashtag_final\n        # now substitute all \"successful\" converted hashtags, delete not successful\n        for i, correct_flag in enumerate(corrects):\n            to_replace = \"#\" + original_hashtags[i]\n            if correct_flag:\n                x = x.replace(to_replace, hashtags[i])\n            else:\n                x = x.replace(to_replace, \"\")\n\n    return x","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lowercase\ndef text_lowercase(x):\n    return x.lower()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install emot","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\nfrom emot.emo_unicode import UNICODE_EMOJI # For emojis\n\n# Function for converting emojis into word - it takes a bit\ndef convert_emojis(text):\n    for emot in UNICODE_EMOJI:\n        text = text.replace(emot, \"_\".join(UNICODE_EMOJI[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n        text = text.replace(\"_\", \" \")\n    return text","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from emot.emo_unicode import EMOTICONS_EMO # For EMOTICONS\n\n# Function for converting emoticons into word\ndef convert_emoticons(text):\n    for emot in EMOTICONS_EMO:\n        text = text.replace(emot, \"_\".join(EMOTICONS_EMO[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n        text = text.replace(\"_\", \" \")\n    return text","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import string\n\n# Remove useless spaces, numbers\ndef rem_spaces_numbers(text):\n    # remove numbers\n    mod_text = re.sub(r'\\d+', ' ', text)\n    # remove useless spaces (more than one spaces concatenated and spaces at beggining/end of a text)\n    mod_text = re.sub(' +', ' ', mod_text)\n    mod_text = re.sub('^ +', '', mod_text)\n    mod_text = re.sub(' +$', '', mod_text)\n\n    return mod_text","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install textblob\n\nfrom textblob import TextBlob\n\ndef correct_spelling(text):\n    textBlb = TextBlob(text)\n    textCorrected = textBlb.correct()   # Correcting the text\n    return textCorrected","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert slang into actual words\nabbreviations = {\n'im': \"I am\",\n\"ur\":\"You are\",\n'atm': 'At The Moment',\n'afaik': 'As Far as I Know',\n'b/c': 'Because',\n'bfn': 'Bye For Now',\n'br': 'Best Regards',\n'btw': 'By the Way',\n'dm': 'Direct Message',\n'em': 'email',\n'fb': 'facebook',\n'ff': 'Follow Friday',\n'ffs': \"For Fuck's Sake\",\n'fml': 'Fuck My Life',\n'ftf': 'Face To Face',\n'ftl': 'For The Loss',\n'ftw': 'For The Win',\n'fwd': 'Forward',\n'fwiw': \"For What It's Worth\",\n'ht': 'Hat Tip',\n'hth': 'Hope That Helps',\n'imho': 'In My Humble Opinion',\n'imo': 'In My Opinion',\n'irl': 'In Real Life',\n'jv': 'Joint Venture',\n'j/k': 'Just Kidding',\n'li': 'LinkedIn',\n'lmk': 'Let Me Know',\n'lol': 'Laughing Out Loud',\n'mt': 'Modified Tweet',\n'nsfw': 'Not Safe For Work',\n'oh': 'Overheard',\n'omfg': 'Oh My Fucking God',\n'omg': 'Oh My God',\n'prt': 'Partial Retweet',\n're': 'Replies',\n'rr': 'Re-Run',\n'rt': 'Retweet',\n'rtf': 'Read The FAQ',\n'rtfm': 'Read The Fucking Manual',\n'rthx': 'Thanks For The Retweet',\n'snafu': 'Situation Normal All Fucked Up',\n'sob': 'Son Of a Bitch',\n'stfu': 'Shut the Fuck Up',\n\"smh\": \"Shake My Head\",\n'tmb': 'Tweet Me Back',\n'tmi': 'Too Much Information',\n'wtf': 'What The Fuck',\n'ymmv': 'Your Mileage May Vary',\n'yw': \"You're Welcome\",\n'tl;DR': \"Too Long Dind't Read\",\n\"$\": \"Dollar\",\n\"€\": \"Euro\",\n\"4ao\": \"For Adults Only\",\n\"a.m\": \"Before Midday\",\n\"a3\": \"Anytime Anywhere Anyplace\",\n\"aamof\": \"As A Matter Of Fact\",\n\"acct\": \"Account\",\n\"adih\": \"Another Day In Hell\",\n\"afaic\": \"As Far As I Am Concerned\",\n\"afaict\": \"As Far As I Can Tell\",\n\"afaik\": \"As Far As I Know\",\n\"afair\": \"As Far As I Remember\",\n\"afk\": \"Away From Keyboard\",\n\"app\": \"Application\",\n\"approx\": \"Approximately\",\n\"apps\": \"Applications\",\n\"asap\": \"As Soon As Possible\",\n\"asl\": \"Age, Sex, Location\",\n\"atk\": \"At The Keyboard\",\n\"ave.\": \"Avenue\",\n\"aymm\": \"Are You My Mother\",\n\"ayor\": \"At Your Own Risk\",\n\"b&b\": \"Bed And Breakfast\",\n\"b+b\": \"Bed And Breakfast\",\n\"b.c\": \"Before Christ\",\n\"b2b\": \"Business To Business\",\n\"b2c\": \"Business To Customer\",\n\"b4\": \"Before\",\n\"b4n\": \"Bye For Now\",\n\"b@u\": \"Back At You\",\n\"bae\": \"Before Anyone Else\",\n\"bak\": \"Back At Keyboard\",\n\"bbbg\": \"Bye Bye Be Good\",\n\"bbc\": \"British Broadcasting Corporation\",\n\"bbias\": \"Be Back In A Second\",\n\"bbl\": \"Be Back Later\",\n\"bbs\": \"Be Back Soon\",\n\"be4\": \"Before\",\n\"bfn\": \"Bye For Now\",\n\"blvd\": \"Boulevard\",\n\"bout\": \"About\",\n\"brb\": \"Be Right Back\",\n\"bros\": \"Brothers\",\n\"brt\": \"Be Right There\",\n\"bsaaw\": \"Big Smile And A Wink\",\n\"btw\": \"By The Way\",\n\"bwl\": \"Bursting With Laughter\",\n\"c/o\": \"Care Of\",\n\"cet\": \"Central European Time\",\n\"cf\": \"Compare\",\n\"cia\": \"Central Intelligence Agency\",\n\"csl\": \"Can Not Stop Laughing\",\n\"cu\": \"See You\",\n\"cul8r\": \"See You Later\",\n\"cv\": \"Curriculum Vitae\",\n\"cwot\": \"Complete Waste Of Time\",\n\"cya\": \"See You\",\n\"cyt\": \"See You Tomorrow\",\n\"dae\": \"Does Anyone Else\",\n\"dbmib\": \"Do Not Bother Me I Am Busy\",\n\"diy\": \"Do It Yourself\",\n\"dm\": \"Direct Message\",\n\"dwh\": \"During Work Hours\",\n\"e123\": \"Easy As One Two Three\",\n\"eet\": \"Eastern European Time\",\n\"eg\": \"Example\",\n\"embm\": \"Early Morning Business Meeting\",\n\"encl\": \"Enclosed\",\n\"encl.\": \"Enclosed\",\n\"etc\": \"And So On\",\n\"faq\": \"Frequently Asked Questions\",\n\"fawc\": \"For Anyone Who Cares\",\n\"fb\": \"Facebook\",\n\"fc\": \"Fingers Crossed\",\n\"fig\": \"Figure\",\n\"fimh\": \"Forever In My Heart\",\n\"ft.\": \"Feet\",\n\"ft\": \"Featuring\",\n\"ftl\": \"For The Loss\",\n\"ftw\": \"For The Win\",\n\"fwiw\": \"For What It Is Worth\",\n\"fyi\": \"For Your Information\",\n\"g9\": \"Genius\",\n\"gahoy\": \"Get A Hold Of Yourself\",\n\"gal\": \"Get A Life\",\n\"gcse\": \"General Certificate Of Secondary Education\",\n\"gfn\": \"Gone For Now\",\n\"gg\": \"Good Game\",\n\"gl\": \"Good Luck\",\n\"glhf\": \"Good Luck Have Fun\",\n\"gmt\": \"Greenwich Mean Time\",\n\"gmta\": \"Great Minds Think Alike\",\n\"gn\": \"Good Night\",\n\"g.o.a.t\": \"Greatest Of All Time\",\n\"goat\": \"Greatest Of All Time\",\n\"goi\": \"Get Over It\",\n\"gps\": \"Global Positioning System\",\n\"gr8\": \"Great\",\n\"gratz\": \"Congratulations\",\n\"gyal\": \"Girl\",\n\"h&c\": \"Hot And Cold\",\n\"hp\": \"Horsepower\",\n\"hr\": \"Hour\",\n\"hrh\": \"His Royal Highness\",\n\"ht\": \"Height\",\n\"ibrb\": \"I Will Be Right Back\",\n\"ic\": \"I See\",\n\"icq\": \"I Seek You\",\n\"icymi\": \"In Case You Missed It\",\n\"idc\": \"I Do Not Care\",\n\"idgadf\": \"I Do Not Give A Damn Fuck\",\n\"idgaf\": \"I Do Not Give A Fuck\",\n\"idk\": \"I Do Not Know\",\n\"ie\": \"That Is\",\n\"i.e\": \"That Is\",\n\"ifyp\": \"I Feel Your Pain\",\n\"ig\": \"Instagram\",\n\"iirc\": \"If I Remember Correctly\",\n\"ilu\": \"I Love You\",\n\"ily\": \"I Love You\",\n\"imho\": \"In My Humble Opinion\",\n\"imo\": \"In My Opinion\",\n\"imu\": \"I Miss You\",\n\"iow\": \"In Other Words\",\n\"irl\": \"In Real Life\",\n\"j4f\": \"Just For Fun\",\n\"jic\": \"Just In Case\",\n\"jk\": \"Just Kidding\",\n\"jsyk\": \"Just So You Know\",\n\"l8r\": \"Later\",\n\"lb\": \"Pound\",\n\"lbs\": \"Pounds\",\n\"ldr\": \"Long Distance Relationship\",\n\"lmao\": \"Laugh My Ass Off\",\n\"lmfao\": \"Laugh My Fucking Ass Off\",\n\"lol\": \"Laughing Out Loud\",\n\"ltd\": \"Limited\",\n\"ltns\": \"Long Time No See\",\n\"m8\": \"Mate\",\n\"mf\": \"Motherfucker\",\n\"mfs\": \"Motherfuckers\",\n\"mfw\": \"My Face When\",\n\"mofo\": \"Motherfucker\",\n\"mph\": \"Miles Per Hour\",\n\"mr\": \"Mister\",\n\"mrw\": \"My Reaction When\",\n\"ms\": \"Miss\",\n\"mte\": \"My Thoughts Exactly\",\n\"nagi\": \"Not A Good Idea\",\n\"nbc\": \"National Broadcasting Company\",\n\"nbd\": \"Not Big Deal\",\n\"nfs\": \"Not For Sale\",\n\"ngl\": \"Not Going To Lie\",\n\"nhs\": \"National Health Service\",\n\"nrn\": \"No Reply Necessary\",\n\"nsfl\": \"Not Safe For Life\",\n\"nsfw\": \"Not Safe For Work\",\n\"nth\": \"Nice To Have\",\n\"nvr\": \"Never\",\n\"nyc\": \"New York City\",\n\"oc\": \"Original Content\",\n\"og\": \"Original\",\n\"ohp\": \"Overhead Projector\",\n\"oic\": \"Oh I See\",\n\"omdb\": \"Over My Dead Body\",\n\"omg\": \"Oh My God\",\n\"omw\": \"On My Way\",\n\"p.a\": \"Per Annum\",\n\"p.m\": \"After Midday\",\n\"pm\": \"Prime Minister\",\n\"poc\": \"People Of Color\",\n\"pov\": \"Point Of View\",\n\"pp\": \"Pages\",\n\"ppl\": \"People\",\n\"prw\": \"Parents Are Watching\",\n\"ps\": \"Postscript\",\n\"pt\": \"Point\",\n\"ptb\": \"Please Text Back\",\n\"pto\": \"Please Turn Over\",\n\"qpsa\": \"What Happens\",\n\"ratchet\": \"Rude\",\n\"rbtl\": \"Read Between The Lines\",\n\"rlrt\": \"Real Life Retweet\",\n\"rofl\": \"Rolling On The Floor Laughing\",\n\"roflol\": \"Rolling On The Floor Laughing Out Loud\",\n\"rotflmao\": \"Rolling On The Floor Laughing My Ass Off\",\n\"rt\": \"Retweet\",\n\"ruok\": \"Are You Ok\",\n\"sfw\": \"Safe For Work\",\n\"sk8\": \"Skate\",\n\"smh\": \"Shake My Head\",\n\"sq\": \"Square\",\n\"srsly\": \"Seriously\",\n\"ssdd\": \"Same Stuff Different Day\",\n\"tbh\": \"To Be Honest\",\n\"tbs\": \"Tablespooful\",\n\"tbsp\": \"Tablespooful\",\n\"tfw\": \"That Feeling When\",\n\"thks\": \"Thank You\",\n\"tho\": \"Though\",\n\"thx\": \"Thank You\",\n\"tia\": \"Thanks In Advance\",\n\"til\": \"Today I Learned\",\n\"tl;dr\": \"Too Long I Did Not Read\",\n\"tldr\": \"Too Long I Did Not Read\",\n\"tmb\": \"Tweet Me Back\",\n\"tntl\": \"Trying Not To Laugh\",\n\"ttyl\": \"Talk To You Later\",\n\"u\": \"You\",\n\"u2\": \"You Too\",\n\"u4e\": \"Yours For Ever\",\n\"utc\": \"Coordinated Universal Time\",\n\"w/\": \"With\",\n\"w/o\": \"Without\",\n\"w8\": \"Wait\",\n\"wassup\": \"What Is Up\",\n\"wb\": \"Welcome Back\",\n\"wtf\": \"What The Fuck\",\n\"wtg\": \"Way To Go\",\n\"wtpa\": \"Where The Party At\",\n\"wuf\": \"Where Are You From\",\n\"wuzup\": \"What Is Up\",\n\"wywh\": \"Wish You Were Here\",\n\"yd\": \"Yard\",\n\"ygtr\": \"You Got That Right\",\n\"ynk\": \"You Never Know\",\n\"zzz\": \"Sleeping Bored And Tired\"\n}\n\n\ndef convert_abbrev_text(text):\n    final_text = \"\"\n    for word in text.split():\n        if word.lower() in abbreviations.keys(): # add lower to find the value based on a cased format \n            final_text += abbreviations[word.lower()] \n        else:\n            final_text += word + \" \"\n    return final_text","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_suspension_points(text):\n    res = re.sub(r'[.]{2,}', \"...\", text)\n    res = re.sub(r'[?]{2,}', \"???\", res)\n    res = re.sub(r'[!]{2,}', \"!!!\", res)\n\n    return res","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_all_at_once(text):\n    text = remove_urls(text)\n    text = remove_html(text)\n    text = remove_mention(text)\n    text = handle_hashtag(text)\n    text = convert_emojis(text)\n    text = convert_emoticons(text)\n    text = convert_suspension_points(text)\n    text = correct_spelling(text)  # expensive\n    text = convert_abbrev_text(text)\n    text = rem_spaces_numbers(text)\n    text = text_lowercase(text)\n    return text","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm.auto import tqdm\ntqdm.pandas()\n\n# APPLY PREPROCESSING TO TRAIN\ntrain_df['preprocessed_text'] = train_df['text'].progress_apply(preprocess_all_at_once)\n\n# APPLY PREPROCESSING TO TEST\ntest_df['preprocessed_text'] = test_df['text'].progress_apply(preprocess_all_at_once)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Map textual label to categorical ( 0:negative, 1:neutral, 2:positive )\n\ndef map_sentiment(sentiment: str):\n    if sentiment == \"positive\":\n        return [0,0,1]\n    if sentiment == \"negative\":\n        return [1,0,0]\n    if sentiment == \"neutral\":\n        return [0,1,0]\n\n    raise ValueError('Value not present')\n\ntrain_df[\"sentiment\"] = train_df[\"sentiment\"].apply(map_sentiment)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df, validation_df = train_test_split(train_df, test_size=0.2, random_state=42, stratify=train_df[\"sentiment\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sentiment Analysis","metadata":{}},{"cell_type":"code","source":"!pip install -q transformers\n!pip install tensorflow-addons\n!pip install emoji==0.6.0","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import BertTokenizer,AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('cardiffnlp/bertweet-base-sentiment')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow_datasets as tfds\nimport tensorflow as tf","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# can be up to 512 for BERT\nmax_length = 128\nbatch_size = 32\n\ndef convert_example_to_feature(text):\n    return tokenizer.encode_plus(text,\n                add_special_tokens = True, # add [CLS], [SEP]\n                max_length = max_length, # max length of the text that can go to BERT\n                pad_to_max_length = True, # add [PAD] tokens\n                return_attention_mask = True, # add attention mask to not focus on pad tokens\n    )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def map_example_to_dict(input_ids, attention_masks, token_type_ids, label):\n    if label is not None:\n        return {\n            \"input_ids\": input_ids,\n            \"token_type_ids\": token_type_ids,\n            \"attention_mask\": attention_masks,\n        }, label\n    return {\n        \"input_ids\": input_ids,\n        \"token_type_ids\": token_type_ids,\n        \"attention_mask\": attention_masks,\n    }","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nimport numpy as np\n\ndef encode_examples(df, labels=True):\n    # prepare list, so that we can build up final TensorFlow dataset from slices.\n    input_ids_list = []\n    token_type_ids_list = []\n    attention_mask_list = []\n    label_list = []\n    for i in tqdm(range(df.shape[0])):\n        text = df.iloc[i][\"preprocessed_text\"]\n        bert_input = convert_example_to_feature(text)\n        input_ids_list.append(bert_input['input_ids'])\n        token_type_ids_list.append(bert_input['token_type_ids'])\n        attention_mask_list.append(bert_input['attention_mask'])\n        if labels:\n            label = df.iloc[i][\"sentiment\"]\n            label_list.append(label)\n  \n  if labels:\n    return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, label_list)).map(map_example_to_dict)\n\n  return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, None)).map(map_example_to_dict)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df_encoded = encode_examples(train_df).batch(batch_size)\nvalidation_df_encoded = encode_examples(validation_df).batch(batch_size)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TFBertForSequenceClassification\nfrom transformers import TFAutoModelForSequenceClassification\nimport tensorflow as tf\nimport tensorflow_addons as tfa\n\n# recommended learning rate for Adam 5e-5, 3e-5, 2e-5\nlearning_rate = 2e-5\n# we will do just 1 epoch, though multiple epochs might be better as long as we will not overfit the model\nnumber_of_epochs = 1\n\n# model initialization\nmodel = TFAutoModelForSequenceClassification.from_pretrained('cardiffnlp/bertweet-base-sentiment', num_labels=3)\n\n# choosing Adam optimizer\noptimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)\n# we do not have one-hot vectors, we can use sparce categorical cross entropy and accuracy\nloss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\nmetric = tf.keras.metrics.CategoricalAccuracy('accuracy')\nmodel.compile(optimizer=optimizer, loss=loss, metrics=[metric])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_history = model.fit(train_df_encoded, epochs=number_of_epochs, validation_data=validation_df_encoded)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df_encoded = []\n\nfor i in tqdm(range(test_df.shape[0])):\n    test_df_encoded.append(tokenizer.encode(test_df.iloc[i][\"text\"],truncation=True,padding=True,return_tensors=\"tf\"))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf_output = []\n\nfor i in tqdm(range(len(test_df_encoded))):\n    tf_output.append(model.predict(test_df_encoded[i])[0])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf_output_flattened = list(map(lambda x: x.flatten(),tf_output))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf_output_np = np.array(tf_output_flattened)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf_prediction = tf.nn.softmax(tf_output_np, axis=1)\nlabels = tf.argmax(tf_prediction, axis=1)\nlabels = labels.numpy()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prediction are in {0,1,2}, challenge format is {-1,0,1}\ntest_df[\"sentiment\"] = labels - 1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> # Interpretability","metadata":{}},{"cell_type":"code","source":"def get_gradients(text, model, tokenizer):\n\n    def get_correct_span_mask(correct_index, token_size):\n        span_mask = np.zeros((1, token_size))\n        span_mask[0, correct_index] = 1\n        span_mask = tf.constant(span_mask, dtype='float32')\n        return span_mask\n\n    embedding_matrix = model.roberta.embeddings.weights[0]\n    encoded_tokens = tokenizer(text, return_tensors=\"tf\")\n    token_ids = list(encoded_tokens[\"input_ids\"].numpy()[0])\n    vocab_size = embedding_matrix.get_shape()[0]\n\n    # convert token ids to one hot. We can't differentiate wrt to int token ids hence the need for one hot representation\n    token_ids_tensor = tf.constant([token_ids], dtype='int32')\n    token_ids_tensor_one_hot = tf.one_hot(token_ids_tensor, vocab_size)\n\n    with tf.GradientTape(watch_accessed_variables=False) as tape:\n        # (i) watch input variable\n        tape.watch(token_ids_tensor_one_hot)\n\n        # multiply input model embedding matrix; allows us do backprop wrt one hot input\n        inputs_embeds = tf.matmul(token_ids_tensor_one_hot,embedding_matrix)\n\n        # (ii) get prediction\n        pred_scores = model({\"inputs_embeds\": inputs_embeds, \"attention_mask\": encoded_tokens[\"attention_mask\"] } ).logits\n        max_class = tf.argmax(pred_scores, axis=1).numpy()[0]\n\n        # get mask for predicted score class\n        score_mask = get_correct_span_mask(max_class, pred_scores.shape[1])\n\n        # zero out all predictions outside of the correct  prediction class; we want to get gradients wrt to just this class\n        predict_correct_class = tf.reduce_sum(pred_scores * score_mask )\n\n        # (iii) get gradient of input with respect to prediction class\n        gradient_non_normalized = tf.norm(\n            tape.gradient(predict_correct_class, token_ids_tensor_one_hot),axis=2)\n\n        # (iv) normalize gradient scores and return them as \"explanations\"\n        gradient_tensor = (\n            gradient_non_normalized /\n            tf.reduce_max(gradient_non_normalized)\n        )\n        gradients = gradient_tensor[0].numpy().tolist()\n        token_words = tokenizer.convert_ids_to_tokens(token_ids)\n\n        prediction_label= max_class\n    return gradients, token_words , prediction_label","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_gradients(tokens,gradients, title, label):\n    \"\"\" Plot  explanations\n    \"\"\"\n    plt.figure(figsize=(21,3))\n    xvals = [ x + str(i) for i,x in enumerate(tokens)]\n\n    if label == 'Negative' : \n        colors =  [ (1,0,0, c) for c in (gradients) ]\n    elif label == 'Neutral' : \n        colors =  [ (1,1,0, c) for c in (gradients) ]\n    else : \n        colors =  [ (0,1,0, c) for c in (gradients) ]\n    plt.rcParams['axes.facecolor'] = 'white'\n    plt.tight_layout()\n    plt.bar(xvals, gradients, color=colors, linewidth=1 )\n    plt.xlabel(\"Token\", fontsize=25 , color='black')\n    plt.ylabel(\"Explainability score\", fontsize=25, color='black')\n    plt.xticks(fontsize=20, color='black')\n    plt.yticks(fontsize=20, color='black')\n    plt.title(title, fontsize=30)\n    plt.xticks(ticks=[i for i in range(len(tokens))], labels=tokens, fontsize=20,rotation=90)\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"texts = test_df[\"text\"]\n\nresults_interpretability = []\nfor i in tqdm(range(len(texts))):\n    text = texts[i]\n\n    gradients, words, label = get_gradients(text, model, tokenizer)\n    results_interpretability.append(\n      {\"sentence\": text,\n      \"words\": words,\n       \"label\": label,\n      \"gradients\": gradients}\n    )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results_interpretability_df = pd.DataFrame(results_interpretability)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nfrom tqdm.notebook import tqdm\nimport ast\ntqdm.pandas()\n\ndef find_indices(lst, condition):\n  return [i for i, elem in enumerate(lst) if condition(elem)]\n\n\ndef unite_tokens(row):\n  words = ast.literal_eval(row[\"words\"])\n  sentence = row[\"sentence\"]\n  gradients = ast.literal_eval(row[\"gradients\"])\n  label = row[\"label\"]\n\n  indices_chiocciole = find_indices(words, lambda x: \"@@\" in x) \n\n  new_indices = []\n\n  for index in indices_chiocciole:\n    new_indices.append(index)\n    new_indices.append(index + 1)\n\n  # indices of words that must be united and averaged interpretability score\n  new_indices = list(set(new_indices))\n\n  new_words = []\n  new_gradients = []\n  \n  continuo = False\n  j = 0\n  for i, word in enumerate(words):\n    if i in new_indices:\n      if continuo == False:\n        new_words.append(word.replace(\"@@\", \"\"))\n        new_gradients.append(gradients[i])\n        continuo = True\n      else:\n        new_words[-1] += word.replace(\"@@\", \"\")\n        new_gradients[-1] = max(gradients[i], new_gradients[-1])\n    else:\n      if continuo:\n        continuo = False\n      new_words.append(word)\n      new_gradients.append(gradients[i])\n\n  new_row = pd.Series(data=[new_words, sentence,  label, new_gradients], index=['words', 'sentence', 'label','gradients'])\n\n  return new_row\n\n\nresults_interpretability_df = pd.read_csv(f\"/content/drive/MyDrive/AML/Challenge_3/results_interpretability_df.csv\")\n\nresults_processed_interpretability_df = results_interpretability_df.progress_apply(unite_tokens, axis=1)\n\nresults_processed_interpretability_df.to_csv(f\"/content/drive/MyDrive/AML/Challenge_3/results_processed_interpretability_df.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ast\ndef get_test_selected_text(row):\n    row.words = ast.literal_eval(row.words)\n    row.gradients = ast.literal_eval(row.gradients)\n    return row\n\nresults_processed_interpretability_df = pd.read_csv(\"results_processed_interpretability_df.csv\")\nresults_processed_interpretability_df = results_processed_interpretability_df.progress_apply(get_test_selected_text, axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentiment_dict = {\n    0: \"Negative\",\n    1: \"Neutral\",\n    2: \"Positive\"\n}\n\nfor i in range(len(results_processed_interpretability_df.iloc[:10,:])):\n    row = results_processed_interpretability_df.iloc[i,:]\n    words = row[\"words\"]\n    gradients = row[\"gradients\"]\n    label = sentiment_dict[row[\"label\"]]\n    sentence = row[\"sentence\"]\n \n    plot_gradients(words, gradients, f\"Prediction: {label} | {sentence} \", label)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu\nfrom collections import defaultdict\n\n# BLEU SCORE\n\nbleu_scores = defaultdict(list)\n\nthresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\nfor threshold in thresholds:\n    for i in range(len(results_processed_interpretability_df)):\n        row = results_processed_interpretability_df.iloc[i,:]\n        words = row[\"words\"]\n        gradients = row[\"gradients\"]\n        label = sentiment_dict[row[\"label\"]]\n        sentence = row[\"sentence\"]\n\n        # print(gradients)\n        # print(f\"words: {words}\")\n\n        mask = np.array(gradients) > threshold\n        # print(f\"mask: {mask}\")\n\n        words = [ np.array(words)[mask].tolist() ]\n\n        # print(f\"words: {words}\")\n        # print(f\"test split: {test_df.iloc[i]['selected_text'].split(' ')}\")\n        bleu_score = sentence_bleu(words, test_df.iloc[i][\"selected_text\"].split(\" \"))\n\n        # print(f\"score: {bleu_score}\")\n\n        bleu_scores[threshold].append(bleu_score)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\n\nplt.rcParams['axes.facecolor'] = 'white'\nfor threshold in thresholds:\n    sns.displot(bleu_scores[threshold])\n    plt.title(f'Threshold used: {threshold}', fontsize=20)\n    plt.xticks(color=\"black\")\n    plt.yticks(color=\"black\")\n    plt.xlabel(\"Score\", fontsize=15 , color='black')\n    plt.ylabel(\"Count\", fontsize=15, color='black')\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> # Results analysis","metadata":{}},{"cell_type":"code","source":"validation_df_encoded_no_batch = []\n\nfor i in tqdm(range(validation_df.shape[0])):\n    validation_df_encoded_no_batch.append(tokenizer.encode(validation_df.iloc[i][\"text\"],truncation=True,padding=True,return_tensors=\"tf\"))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf_output_validation = []\n\nfor i in tqdm(range(len(validation_df_encoded_no_batch))):\n     tf_output_validation.append(model.predict(validation_df_encoded_no_batch[i])[0])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf_output_val_flattened = list(map(lambda x: x.flatten(),tf_output_validation))\n\ntf_output_val_np = np.array(tf_output_val_flattened)\n\ntf_val_prediction = tf.nn.softmax(tf_output_val_np, axis=1)\nlabels_val = tf.argmax(tf_val_prediction, axis=1)\nlabels_val = labels_val.numpy()\n\nlabels_val = labels_val - 1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_with_error = []\nerror_indexes = []\n\ny_val = validation_df[\"sentiment\"].apply(np.argmax) - 1\n\ny_val_np = y_val.values\ncount_error = 0\nfor i in range(len(labels_val)):\n    if(labels_val[i] !=  y_val_np[i]):\n        count_error += 1\n        data_with_error.append([ validation_df[\"text\"].iloc[i], labels_val[i], y_val_np[i]])\n        error_indexes.append(i)\n        print(validation_df[\"text\"].iloc[i])\n\nprint(f\"Accuracy: {(1 - count_error/len(labels_val)):.2f}\")\n\ndf_data_with_error = pd.DataFrame(data_with_error, columns=[\"text\",\"y pred\",\"y true\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\nclass_names = [\"negative\",\"neutral\",\"positive\"]\n\ndef show_confusion_matrix(confusion_matrix):\n    hmap = sns.heatmap(confusion_matrix, annot=True, cmap=\"Blues\")\n    hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n    hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n    plt.ylabel('True sentiment', fontsize=15, color='black')\n    plt.xlabel('Predicted sentiment', fontsize=15, color='black')\n    plt.xticks(color='black')\n    plt.yticks(color='black')\n    plt.title('Confusion Matrix', fontsize=20)\n\ncm = confusion_matrix(y_val_np, labels_val, normalize='true')\ndf_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\nshow_confusion_matrix(df_cm)","metadata":{},"execution_count":null,"outputs":[]}]}